# app.py
# --- Ù…Ø³Ø§Ø¹Ø¯ Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ "Ù…Ø§ÙŠÙ†Ø¯ Ù…Ø§Ø³ØªØ±" (Ù†Ø³Ø®Ø© Ø®ÙÙŠÙØ© Ù„Ù„Ù…Ù†ØµØ§Øª Ø§Ù„Ù…Ø¬Ø§Ù†ÙŠØ©) ---
# --- Ù…ØµÙ…Ù… Ù„Ù„Ø¹Ù…Ù„ Ø¹Ù„Ù‰ Ø®Ø·Ø·Ø· Ù…Ø«Ù„ Render Free Plan ---

import gradio as gr
import torch
import random
import time
import os
import re
from huggingface_hub import hf_hub_download
from llama_cpp import Llama
from duckduckgo_search import DDGS
import warnings
warnings.filterwarnings("ignore")

# --- Ù…ØªØºÙŠØ±Ø§Øª Ø¹Ø§Ù…Ø© ---
chat_llm = None
image_pipe = None
chat_model_loaded = False
image_model_loaded = False

# --- Ø¯Ø§Ù„Ø© ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø¯Ø±Ø¯Ø´Ø© (ØªÙØ³ØªØ¯Ø¹Ù‰ Ù…Ø±Ø© ÙˆØ§Ø­Ø¯Ø© Ø¹Ù†Ø¯ Ø£ÙˆÙ„ Ø·Ù„Ø¨) ---
def load_chat_model():
    global chat_llm, chat_model_loaded
    if chat_model_loaded:
        return
    try:
        print("ğŸ”„ Ø¬Ø§Ø±ÙŠ ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø¯Ø±Ø¯Ø´Ø© (Ù‚Ø¯ ÙŠØ³ØªØºØ±Ù‚ Ù‡Ø°Ø§ Ø¯Ù‚ÙŠÙ‚Ø© ÙˆØ§Ø­Ø¯Ø©)...")
        # Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù†Ù…ÙˆØ°Ø¬ Ø£ØµØºØ± ÙˆØ£Ø®Ù ÙˆØ²Ù†Ø§Ù‹ Ù„ØªÙˆÙÙŠØ± Ø§Ù„Ø°Ø§ÙƒØ±Ø©
        model_id = "TheBloke/TinyLlama-2-1.1B-Chat-v1.0-GGUF"
        model_file = "tinyllama-2-1.1b-chat-v1.0.Q4_K_M.gguf"
        model_path = hf_hub_download(repo_id=model_id, filename=model_file)
        chat_llm = Llama(model_path=model_path, n_ctx=2048, n_gpu_layers=0, verbose=False)
        chat_model_loaded = True
        print("âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø¯Ø±Ø¯Ø´Ø© Ø¨Ù†Ø¬Ø§Ø­!")
    except Exception as e:
        print(f"âŒ Ø®Ø·Ø£ ÙÙŠ ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø¯Ø±Ø¯Ø´Ø©: {e}")
        chat_model_loaded = False

# --- Ø¯Ø§Ù„Ø© ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„ØµÙˆØ± (ØªÙØ³ØªØ¯Ø¹Ù‰ Ù…Ø±Ø© ÙˆØ§Ø­Ø¯Ø© Ø¹Ù†Ø¯ Ø£ÙˆÙ„ Ø·Ù„Ø¨) ---
def load_image_model():
    global image_pipe, image_model_loaded
    if image_model_loaded:
        return
    try:
        print("ğŸ¨ Ø¬Ø§Ø±ÙŠ ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ ØªÙˆÙ„ÙŠØ¯ Ø§Ù„ØµÙˆØ± (Ù‚Ø¯ ÙŠØ³ØªØºØ±Ù‚ Ù‡Ø°Ø§ Ø¨Ø¶Ø¹ Ø¯Ù‚Ø§Ø¦Ù‚)...")
        # Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù†Ù…ÙˆØ°Ø¬ Ø£ØµØºØ± ÙˆØ£Ø³Ø±Ø¹ Ù„Ù„ØµÙˆØ±
        image_pipe = StableDiffusionPipeline.from_pretrained("stabilityai/sd-turbo", torch_dtype=torch.float16, variant="fp16")
        image_pipe.to("cpu")
        image_model_loaded = True
        print("âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„ØµÙˆØ± Ø¨Ù†Ø¬Ø§Ø­!")
    except Exception as e:
        print(f"âŒ Ø®Ø·Ø£ ÙÙŠ ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„ØµÙˆØ±: {e}")
        image_model_loaded = False

# --- Ø¯ÙˆØ§Ù„ Ø§Ù„Ø®Ø¯Ù…Ø§Øª ---
def perform_web_search(query):
    try:
        with DDGS() as ddgs:
            results = [r for r, _ in ddgs.text(query, max_results=3)]
        return "\n".join(results)
    except Exception as e:
        return f"ÙØ´Ù„ Ø§Ù„Ø¨Ø­Ø«: {e}"

def generate_text(message, history, search_enabled, creativity_level):
    if not chat_model_loaded:
        load_chat_model() # Ø­Ø§ÙˆÙ„ Ø§Ù„ØªØ­Ù…ÙŠÙ„ Ø¥Ø°Ø§ Ù„Ù… ÙŠÙƒÙ† Ù…Ø­Ù…Ù„Ø§Ù‹
        if not chat_model_loaded:
            return "Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø¯Ø±Ø¯Ø´Ø© ØºÙŠØ± Ù…ØªÙˆÙØ± Ø­Ø§Ù„ÙŠØ§Ù‹.", history

    # Ø±Ø³Ø§Ù„Ø© Ù†Ø¸Ø§Ù… Ù…Ø¨Ø³Ø·Ø©
    prompt = f"
